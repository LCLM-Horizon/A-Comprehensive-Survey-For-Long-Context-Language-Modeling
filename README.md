# LCLM-Survey

<div align="center">
 <p align="center">
 
   <a href="#1-Survey-Papers">üìù Papers</a> | 
 
 </p>
</div>
<div align="center">

<!-- ![Build](https://img.shields.io/appveyor/build/gruntjs/grunt) -->
[![LICENSE](https://img.shields.io/github/license/LCLM-Space/LCLM-Survey)](https://github.com/LCLM-Space/LCLM-Survey/blob/main/LICENSE)
![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
[![commit](https://img.shields.io/github/last-commit/LCLM-Space/LCLM-Survey?color=blue)](https://github.com/LCLM-Space/LCLM-Survey/commits/main)
[![PR](https://img.shields.io/badge/PRs-Welcome-red)](https://github.com/LCLM-Space/LCLM-Survey/pulls)
[![GitHub Repo stars](https://img.shields.io/github/stars/LCLM-Space/LCLM-Survey)](https://github.com/LCLM-Space/LCLM-Survey)
<!-- ![license](https://img.shields.io/bower/l/bootstrap?style=plastic) -->

</div>

> A collection of papers and resources related to Long Context Language Modeling. 
>
> We appreciate any useful suggestions for improvement of this paper list or survey from peers. Please raise issues or send an email to dwzhu@pku.edu.cn or ljh411989@alibaba-inc.com or huanxuanliao@gmail.com. Thanks for your cooperation!
>
> If you find our survey useful for your research, please cite the following paper:

```bibtex

```

## Updates

- [2025.03.09] We release the first version of the survey on Long Context Language Modeling [[**arXiv**]()]


## Table of Contents

- [LCLM-Survey](#lclm-survey)
  - [Updates](#updates)
  - [Table of Contents](#table-of-contents)
  - [Paper List](#paper-list)
    - [Data](#data)
      - [Pretraining](#pretraining)
      - [Posttraining](#posttraining)
    - [Model](#model)
      - [Position Embeddings](#position-embeddings)
      - [Architecture](#architecture)
      - [Hybrid Architecture](#hybrid-architecture)
    - [Inference](#inference)
      - [Sparse Attention (inference)](#sparse-attention-inference)
      - [Prompt Compression](#prompt-compression)
        - [Hard Prompt Compression](#hard-prompt-compression)
        - [Soft Prompt Compression](#soft-prompt-compression)
      - [Memory-Based](#memory-based)
      - [RAG-Based](#rag-based)
      - [Agent-Based](#agent-based)
    - [Evaluation](#evaluation)
      - [Long-Context Comprehension](#long-context-comprehension)
      - [Long-Form Generation](#long-form-generation)
    - [AI Infrastructure](#ai-infrastructure)
      - [Training](#training)
        - [I/O Optimization](#io-optimization)
        - [Memory Optimization](#memory-optimization)
        - [Communication Optimization](#communication-optimization)
      - [Inference](#inference-1)
        - [Quantization](#quantization)
        - [Memory Management](#memory-management)
        - [Prefilling-Decoding Disaggregated Architecture](#prefilling-decoding-disaggregated-architecture)
        - [GPU-CPU Parallel Inference](#gpu-cpu-parallel-inference)
        - [Speculative Decoding](#speculative-decoding)
    - [Interpretability](#interpretability)
      - [Performance Analysis](#performance-analysis)
      - [Model Structure Analysis](#model-structure-analysis)
      - [Visualization and Interpretation of Positional Encoding](#visualization-and-interpretation-of-positional-encoding)
    - [Application](#application)
      - [Agent](#agent)
      - [RAG](#rag)
      - [Chatbot](#chatbot)
      - [Code](#code)
      - [NLP Tasks](#nlp-tasks)
      - [Multimodal Tasks](#multimodal-tasks)
      - [Specific Domains](#specific-domains)
    - [Future Directions](#future-directions)
  - [The Team](#the-team)
  - [Acknowledgments](#acknowledgments)

## Paper List

### Data

#### Pretraining

#### Posttraining

### Model

#### Position Embeddings

#### Architecture

#### Hybrid Architecture

### Inference

#### Sparse Attention (inference)

#### Prompt Compression

##### Hard Prompt Compression

##### Soft Prompt Compression

1. [**Learning to Compress Prompts with Gist Tokens.**](https://openreview.net/forum?id=2DtxPCL3T5) *Jesse Mu, Xiang Lisa Li, Noah Goodman.* NeurIPS 2023. &nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/jayelm/gisting)](https://github.com/jayelm/gisting)


#### Memory-Based

#### RAG-Based

#### Agent-Based

### Evaluation

#### Long-Context Comprehension

#### Long-Form Generation

### AI Infrastructure

#### Training

##### I/O Optimization

##### Memory Optimization

##### Communication Optimization

#### Inference

##### Quantization

##### Memory Management

##### Prefilling-Decoding Disaggregated Architecture

##### GPU-CPU Parallel Inference

##### Speculative Decoding

### Interpretability

#### Performance Analysis

#### Model Structure Analysis

#### Visualization and Interpretation of Positional Encoding

### Application

#### Agent

#### RAG

#### Chatbot

#### Code

#### NLP Tasks

#### Multimodal Tasks

#### Specific Domains

### Future Directions

## The Team

## Acknowledgments

Please contact us if We miss your names in the list, I will add you back ASAP!